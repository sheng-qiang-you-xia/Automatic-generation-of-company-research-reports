"""
é‡‘èç ”æŠ¥ç”Ÿæˆå™¨
æ•´åˆè´¢åŠ¡åˆ†æã€è‚¡æƒåˆ†æã€ä¼°å€¼æ¨¡å‹å’Œè¡Œä¸šä¿¡æ¯ï¼Œç”Ÿæˆå®Œæ•´çš„é‡‘èç ”æŠ¥
"""

import os
import glob
import time
import json
import requests
from datetime import datetime
from dotenv import load_dotenv
import importlib

from data_analysis_agent import quick_analysis
from data_analysis_agent.config.llm_config import LLMConfig
from data_analysis_agent.utils.llm_helper import LLMHelper
from utils.get_shareholder_info import get_shareholder_info, get_table_content
from utils.get_financial_statements import get_all_financial_statements, save_financial_statements_to_csv
from utils.identify_competitors import identify_competitors_with_ai
from utils.get_stock_intro import get_stock_intro, save_stock_intro_to_txt
import logging


#é…ç½®æ—¥å¿—è®°å½•
os.makedirs("logs",exist_ok=True)
logging.basicConfig(
    level = logging.INFO,
    format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s', 
    filename = f"logs/research_report.log",
    filemode = "w",
    encoding = "utf-8"
)
logger = logging.getLogger(__name__)#åˆ›å»ºæ—¥å¿—è®°å½•å™¨

def open_debug_mode():
    import debugpy
    try:
        # 5678 is the default attach port in the VS Code debug configurations. Unless a host and port are specified, host defaults to 127.0.0.1
        debugpy.listen(("localhost", 5678))
        print("Waiting for debugger attach")
        debugpy.wait_for_client()
    except Exception as e:
        pass

open_debug_mode()

class SearchEngine:
    """æœç´¢å¼•æ“ç±» - ä½¿ç”¨ç™¾åº¦æœç´¢"""
    def __init__(self, engine_type="baidu"):
        self.engine_type = engine_type
    def search(self, keywords, max_results=10):
        """æ‰§è¡Œæœç´¢ - ä½¿ç”¨ç™¾åº¦æœç´¢"""
        return self._baidu_search(keywords, max_results)
    def _baidu_search(self, keywords, max_results):
        """ç™¾åº¦æœç´¢ï¼ˆä½¿ç”¨requestsç›´æ¥è°ƒç”¨ï¼‰"""
        try:
            import urllib.parse
            search_url = "https://www.baidu.com/s"
            params = {
                'wd': keywords,
                'rn': max_results
            }           
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            response = requests.get(search_url, params=params, headers=headers, timeout=30)
            if response.status_code == 200:
                # ç®€å•è§£æHTMLç»“æœ
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(response.text, 'html.parser')
                results = []
                
                for result in soup.find_all('h3', class_='t', limit=max_results):
                    link = result.find('a')
                    if link:
                        title = link.get_text(strip=True)
                        href = link.get('href', '')
                        # è·å–æ‘˜è¦
                        summary_elem = result.find_next_sibling('div', class_='c-abstract')
                        summary = summary_elem.get_text(strip=True) if summary_elem else f"å…³äº {keywords} çš„æœç´¢ç»“æœ"
                        
                        results.append({
                            "title": title,
                            "href": href,
                            "body": summary
                        })
                
                return results
            else:
                logging.error(f"ç™¾åº¦æœç´¢HTTPé”™è¯¯: {response.status_code}")
                return []
                
        except Exception as e:
            logging.error(f"ç™¾åº¦æœç´¢å¤±è´¥: {e}")
            return []


def robust_search_with_retry(keywords, search_engine, max_retries=3):
    """
    å¥å£®çš„æœç´¢å‡½æ•°ï¼ŒåŒ…å«é‡è¯•æœºåˆ¶
    
    Args:
        keywords: æœç´¢å…³é”®è¯
        search_engine: SearchEngineå®ä¾‹
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    
    Returns:
        list: æœç´¢ç»“æœåˆ—è¡¨ï¼Œå¤±è´¥æ—¶è¿”å›ç©ºåˆ—è¡¨
    """
    for attempt in range(max_retries):
        try:
            logging.info(f"å°è¯•æœç´¢ '{keywords}' (ç¬¬{attempt + 1}æ¬¡)")
            results = search_engine.search(keywords, max_results=10)
            if results:
                logging.info(f"æˆåŠŸè·å– {len(results)} æ¡æœç´¢ç»“æœ")
                return results
            else:
                logging.info("æœç´¢ç»“æœä¸ºç©º")
                return []
        except Exception as e:
            logging.error(f"æœç´¢å¤±è´¥ (ç¬¬{attempt + 1}æ¬¡): {e}")
            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 10  # é€’å¢ç­‰å¾…æ—¶é—´
                logging.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)
            else:
                logging.error(f"æœç´¢ '{keywords}' æœ€ç»ˆå¤±è´¥ï¼Œè·³è¿‡")
                return []

# ========== ç¯å¢ƒå˜é‡ä¸å…¨å±€é…ç½® ==========
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
base_url = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
model = os.getenv("OPENAI_MODEL", "gpt-4")

# æ–°å¢ï¼šé€šè¿‡ç¯å¢ƒå˜é‡æ§åˆ¶æ˜¯å¦è·³è¿‡æœç´¢
SKIP_SEARCH = os.getenv("SKIP_SEARCH", "false").lower() == "true"

target_company = "å•†æ±¤ç§‘æŠ€"
target_company_code = "00020"
target_company_market = "HK"
data_dir = "./download_financial_statement_files"  #å­˜æ”¾å…¬å¸ä¸‰å¤§è¡¨
os.makedirs(data_dir, exist_ok=True)  #åˆ›å»ºç›®å½•

company_info_dir = "./company_info"
os.makedirs(company_info_dir, exist_ok=True)

llm_config = LLMConfig(   #æ¨¡å‹é…ç½®åˆå§‹åŒ–
    api_key=api_key,
    base_url=base_url,
    model=model,
    temperature=0.7,
    max_tokens=8192,  # ä¿®å¤ï¼šä»16384æ”¹ä¸º8192ï¼Œç¬¦åˆAPIé™åˆ¶,qwen-plus
)
llm = LLMHelper(llm_config)

# ========== 1. è·å–ç›®æ ‡å…¬å¸åŠç«äº‰å¯¹æ‰‹çš„è´¢åŠ¡æ•°æ® ==========
# è·å–ç«äº‰å¯¹æ‰‹åˆ—è¡¨
other_companies = identify_competitors_with_ai(api_key=api_key,
                                               base_url=base_url,
                                               model_name=model,
                                               company_name=target_company)

#è¿‡æ»¤æ‰æœªä¸Šå¸‚çš„å…¬å¸----ç«äº‰å¯¹æ‰‹ä¸­ä»…å…³æ³¨æœªä¸Šå¸‚å…¬å¸
listed_companies = [company for company in other_companies if company.get('market') != "æœªä¸Šå¸‚"]

# è·å–ç›®æ ‡å…¬å¸è´¢åŠ¡æ•°æ®  --------ä»akshareä¸­è·å–ä¸‰å¤§è¡¨ï¼Œè¾“å‡ºæ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«ä¸‰ä¸ªé”®å€¼å¯¹ï¼Œåˆ†åˆ«æ˜¯èµ„äº§è´Ÿå€ºè¡¨ã€åˆ©æ¶¦è¡¨ã€ç°é‡‘æµé‡è¡¨
print("\n" + "="*80)
print(f"è·å–ç›®æ ‡å…¬å¸ {target_company}({target_company_market}:{target_company_code}) çš„è´¢åŠ¡æ•°æ®")
target_financials = get_all_financial_statements(
    stock_code=target_company_code,
    market=target_company_market,
    period="å¹´åº¦",
    verbose=False
)

save_financial_statements_to_csv(
    financial_statements=target_financials,
    stock_code=target_company_code,
    market=target_company_market,
    company_name=target_company,
    period="å¹´åº¦",
    save_dir=data_dir
)

# è·å–ç«äº‰å¯¹æ‰‹çš„è´¢åŠ¡æ•°æ®
print("\n" + "="*80)
print("è·å–ç«äº‰å¯¹æ‰‹çš„è´¢åŠ¡æ•°æ®")
competitors_financials = {}
for company in listed_companies:
    company_name = company.get('name')
    company_code = company.get('code')
    market_str = company.get('market', '')
    if "A" in market_str:
        market = "A"
        if not (company_code.startswith('SH') or company_code.startswith('SZ')):
            if company_code.startswith('6'):
                company_code = f"SH{company_code}"
            else:
                company_code = f"SZ{company_code}"
    elif "æ¸¯" in market_str:
        market = "HK"
    print(f"\nè·å–ç«äº‰å¯¹æ‰‹ {company_name}({market}:{company_code}) çš„è´¢åŠ¡æ•°æ®")
    try:
        company_financials = get_all_financial_statements(
            stock_code=company_code,
            market=market,
            period="å¹´åº¦",
            verbose=False
        )
        save_financial_statements_to_csv(
            financial_statements=company_financials,
            stock_code=company_code,
            market=market,
            company_name=company_name,
            period="å¹´åº¦",
            save_dir=data_dir
        )
        competitors_financials[company_name] = company_financials
        time.sleep(2)
    except Exception as e:
        print(f"è·å– {company_name} è´¢åŠ¡æ•°æ®å¤±è´¥: {e}")
print("\n" + "="*80)
print("è´¢åŠ¡æ•°æ®è·å–å®Œæˆ")
print(f"ç›®æ ‡å…¬å¸: {target_company}")
print(f"ç«äº‰å¯¹æ‰‹æ•°é‡: {len(competitors_financials)}")
print("="*80)

# ========== 1.1 è·å–æ‰€æœ‰å…¬å¸åŸºç¡€ä¿¡æ¯å¹¶ä¿å­˜ ==========
print("="*80)
print("å¼€å§‹è·å–å…¬å¸åŸºç¡€ä¿¡æ¯")
print("="*80)

# ç»Ÿä¸€æ”¶é›†ç›®æ ‡å…¬å¸ã€ç«äº‰å¯¹æ‰‹ã€ç‰¹å®šå…¬å¸ï¼ˆå¦‚ç™¾åº¦ï¼‰
all_base_info_targets = [(target_company, target_company_code, target_company_market)]
for company in listed_companies:
    company_name = company.get('name')
    company_code = company.get('code')
    market_str = company.get('market', '')
    if "A" in market_str:
        market = "A"
        if not (company_code.startswith('SH') or company_code.startswith('SZ')):
            if company_code.startswith('6'):
                company_code = f"SH{company_code}"
            else:
                company_code = f"SZ{company_code}"
    elif "æ¸¯" in market_str:
        market = "HK"
    all_base_info_targets.append((company_name, company_code, market))
# ç‰¹å®šå…¬å¸å¦‚ç™¾åº¦
all_base_info_targets.append(("ç™¾åº¦", "09888", "HK"))

for company_name, company_code, market in all_base_info_targets:
    logging.info(f"\nè·å–å…¬å¸ {company_name}({market}:{company_code}) çš„åŸºç¡€ä¿¡æ¯")
    company_info = get_stock_intro(company_code, market=market)
    if company_info:
        logging.info(company_info)
        save_path = os.path.join(company_info_dir, f"{company_name}_{market}_{company_code}_info.txt")
        save_stock_intro_to_txt(company_code, market, save_path)
        logging.info(f"å…¬å¸ä¿¡æ¯å·²ä¿å­˜åˆ°: {save_path}")
    else:
        logging.error(f"æœªèƒ½è·å–åˆ° {company_name} çš„åŸºç¡€ä¿¡æ¯")
    time.sleep(1)

# ========== 1.x æœç´¢è¡Œä¸šä¿¡æ¯å¹¶ä¿å­˜ ==========
industry_info_dir = "./industry_info"
os.makedirs(industry_info_dir, exist_ok=True)

logging.info("="*80)
logging.info("å¼€å§‹æœç´¢è¡Œä¸šä¿¡æ¯")
logging.info("="*80)

all_search_results = {}

# æ£€æŸ¥æ˜¯å¦è·³è¿‡æœç´¢ï¼ˆå¦‚æœç½‘ç»œæœ‰é—®é¢˜ï¼‰
if SKIP_SEARCH:
    skip_search = True
    logging.info("æ£€æµ‹åˆ°ç¯å¢ƒå˜é‡ SKIP_SEARCH=trueï¼Œè·³è¿‡ç½‘ç»œæœç´¢")
else:
    # skip_search = input("æ˜¯å¦è·³è¿‡ç½‘ç»œæœç´¢ï¼Ÿ(y/nï¼Œé»˜è®¤n): ").lower().strip() == 'y'?
    skip_search = False

if not skip_search:
    # ç›´æ¥ä½¿ç”¨ç™¾åº¦æœç´¢å¼•æ“
    search_engine = SearchEngine("baidu")
    logging.info("ä½¿ç”¨æœç´¢å¼•æ“: ç™¾åº¦æœç´¢")
    
    # 1. æœç´¢ç›®æ ‡å…¬å¸è¡Œä¸šä¿¡æ¯
    logging.info(f"\næœç´¢ç›®æ ‡å…¬å¸ {target_company} çš„è¡Œä¸šä¿¡æ¯")
    target_search_keywords = f"{target_company} è¡Œä¸šåœ°ä½ å¸‚åœºä»½é¢ ç«äº‰åˆ†æ ä¸šåŠ¡æ¨¡å¼"
    all_search_results[target_company] = robust_search_with_retry(target_search_keywords, search_engine)

    # 2. æœç´¢ç«äº‰å¯¹æ‰‹è¡Œä¸šä¿¡æ¯
    logging.info(f"\næœç´¢ç«äº‰å¯¹æ‰‹çš„è¡Œä¸šä¿¡æ¯")
    for company in listed_companies:
        company_name = company.get('name')
        search_keywords = f"{company_name} è¡Œä¸šåœ°ä½ å¸‚åœºä»½é¢ ä¸šåŠ¡æ¨¡å¼ å‘å±•æˆ˜ç•¥"
        all_search_results[company_name] = robust_search_with_retry(search_keywords, search_engine)
        time.sleep(5)  # çŸ­æš‚ç­‰å¾…ï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
else:
    logging.info("è·³è¿‡ç½‘ç»œæœç´¢ï¼Œä½¿ç”¨ç©ºçš„æœç´¢ç»“æœ...")
    all_search_results = {}

# ä¿å­˜æ‰€æœ‰æœç´¢ç»“æœçš„JSONæ–‡ä»¶
search_results_file = os.path.join(industry_info_dir, "all_search_results.json")
with open(search_results_file, 'w', encoding='utf-8') as f:
    json.dump(all_search_results, f, ensure_ascii=False, indent=2)

# ========== 2. å…¬å¸ä¿¡æ¯æ•´ç† ==========
#å°†ä»akshareä¸­åŒ¹é…åˆ°çš„å…¬å¸åŸºç¡€ä¿¡æ¯ä¸²èµ·æ¥
def get_company_infos(data_dir:str="./company_info"):
    all_files = os.listdir(data_dir)
    company_infos = ""
    for file in all_files:
        if file.endswith(".txt"):
            company_name = file.split(".")[0]
            with open(os.path.join(data_dir, file), 'r', encoding='utf-8') as f:
                content = f.read()
            company_infos += f"ã€å…¬å¸ä¿¡æ¯å¼€å§‹ã€‘\nå…¬å¸åç§°: {company_name}\n{content}\nã€å…¬å¸ä¿¡æ¯ç»“æŸã€‘\n\n"
    return company_infos

company_infos = get_company_infos(company_info_dir)
logger.info(f"å…¬å¸ä¿¡æ¯:{company_infos}")

company_infos = llm.call(
    f"è¯·æ•´ç†ä»¥ä¸‹å…¬å¸ä¿¡æ¯å†…å®¹ï¼Œç¡®ä¿æ ¼å¼æ¸…æ™°æ˜“è¯»ï¼Œå¹¶ä¿ç•™å…³é”®ä¿¡æ¯ï¼š\n{company_infos}",
    system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å…¬å¸ä¿¡æ¯æ•´ç†å¸ˆã€‚",
    max_tokens=8192,  
    temperature=0.7
)
logger.info(f"æ•´ç†åçš„å…¬å¸ä¿¡æ¯:{company_infos}")

# ========== 3. è‚¡æƒä¿¡æ¯æ•´ç† ==========
info = get_shareholder_info()
shangtang_shareholder_info = info.get("tables")
table_content = get_table_content(shangtang_shareholder_info)
shareholder_analysis = llm.call(
    "è¯·åˆ†æä»¥ä¸‹è‚¡ä¸œä¿¡æ¯è¡¨æ ¼å†…å®¹ï¼š\n" + table_content,
    system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è‚¡ä¸œä¿¡æ¯åˆ†æå¸ˆã€‚",
    max_tokens=8192,  # ä¿®å¤ï¼šä»16384æ”¹ä¸º8192
    temperature=0.5
)

# ========== 4. è¡Œä¸šä¿¡æ¯æœç´¢ç»“æœæ•´ç† ==========
with open(search_results_file, 'r', encoding='utf-8') as f:
    all_search_results = json.load(f)
search_res = ""
for company, results in all_search_results.items():
    search_res += f"ã€{company}æœç´¢ä¿¡æ¯å¼€å§‹ã€‘\n"
    for result in results:
        search_res += f"æ ‡é¢˜: {result.get('title', 'æ— æ ‡é¢˜')}\n"
        search_res += f"é“¾æ¥: {result.get('href', 'æ— é“¾æ¥')}\n"
        search_res += f"æ‘˜è¦: {result.get('body', 'æ— æ‘˜è¦')}\n"
        search_res += "----\n"
    search_res += f"ã€{company}æœç´¢ä¿¡æ¯ç»“æŸã€‘\n\n"

# ========== 5. è´¢åŠ¡æ•°æ®åˆ†æä¸å¯¹æ¯”åˆ†æ ==========
def get_company_files(data_dir):
    all_files = glob.glob(f"{data_dir}/*.csv")
    companies = {}
    for file in all_files:
        filename = os.path.basename(file)
        company_name = filename.split("_")[0]
        companies.setdefault(company_name, []).append(file)
    return companies

def analyze_individual_company(company_name, files, llm_config, query=None, verbose=True):
    if query is None:
        query = "åŸºäºè¡¨æ ¼çš„æ•°æ®ï¼Œåˆ†ææœ‰ä»·å€¼çš„å†…å®¹ï¼Œå¹¶ç»˜åˆ¶ç›¸å…³å›¾è¡¨ã€‚æœ€åç”Ÿæˆæ±‡æŠ¥ç»™æˆ‘ã€‚"
    report = quick_analysis(
        query=query, files=files, llm_config=llm_config, 
        absolute_path=True, max_rounds=20
    )
    return report

def format_final_reports(all_reports):
    formatted_output = []
    for company_name, report in all_reports.items():
        formatted_output.append(f"ã€{company_name}è´¢åŠ¡æ•°æ®åˆ†æç»“æœå¼€å§‹ã€‘")
        final_report = report.get("final_report", "æœªç”ŸæˆæŠ¥å‘Š")
        formatted_output.append(final_report)
        formatted_output.append(f"ã€{company_name}è´¢åŠ¡æ•°æ®åˆ†æç»“æœç»“æŸã€‘")
        formatted_output.append("")
    return "\n".join(formatted_output)

# è¯¥å‡½æ•°ç”¨äºæ‰¹é‡åˆ†ææŒ‡å®šç›®å½•ä¸‹æ‰€æœ‰å…¬å¸çš„è´¢åŠ¡æ•°æ®æ–‡ä»¶ã€‚
# å®ƒé¦–å…ˆé€šè¿‡ get_company_files(data_directory) è·å–ç›®å½•ä¸‹æ¯ä¸ªå…¬å¸çš„æ‰€æœ‰CSVæ–‡ä»¶ï¼Œ
# ç„¶åå¯¹æ¯ä¸ªå…¬å¸è°ƒç”¨ analyze_individual_company è¿›è¡Œåˆ†æï¼Œç”Ÿæˆåˆ†ææŠ¥å‘Šã€‚
# æœ€ç»ˆè¿”å›ä¸€ä¸ªå­—å…¸ï¼Œé”®ä¸ºå…¬å¸åï¼Œå€¼ä¸ºå¯¹åº”çš„åˆ†ææŠ¥å‘Šã€‚
def analyze_companies_in_directory(data_directory, llm_config, query="åŸºäºè¡¨æ ¼çš„æ•°æ®ï¼Œåˆ†ææœ‰ä»·å€¼çš„å†…å®¹ï¼Œå¹¶ç»˜åˆ¶ç›¸å…³å›¾è¡¨ã€‚æœ€åç”Ÿæˆæ±‡æŠ¥ç»™æˆ‘ã€‚"):
    company_files = get_company_files(data_directory)
    all_reports = {}
    for company_name, files in company_files.items():
        report = analyze_individual_company(company_name, files, llm_config, query, verbose=False)
        if report:
            all_reports[company_name] = report
    return all_reports

def compare_two_companies(company1_name, company1_files, company2_name, company2_files, llm_config):
    query = "åŸºäºä¸¤ä¸ªå…¬å¸çš„è¡¨æ ¼çš„æ•°æ®ï¼Œåˆ†ææœ‰å…±åŒç‚¹çš„éƒ¨åˆ†ï¼Œç»˜åˆ¶å¯¹æ¯”åˆ†æçš„è¡¨æ ¼ï¼Œå¹¶ç»˜åˆ¶ç›¸å…³å›¾è¡¨ã€‚æœ€åç”Ÿæˆæ±‡æŠ¥ç»™æˆ‘ã€‚"
    all_files = company1_files + company2_files
    report = quick_analysis(
        query=query,
        files=all_files,
        llm_config=llm_config,
        absolute_path=True,
        max_rounds=20
    )
    return report

def run_comparison_analysis(data_directory, target_company_name, llm_config):
    company_files = get_company_files(data_directory)
    if not company_files or target_company_name not in company_files:
        return {}
    competitors = [company for company in company_files.keys() if company != target_company_name]
    comparison_reports = {}
    for competitor in competitors:
        comparison_key = f"{target_company_name}_vs_{competitor}"
        report = compare_two_companies(
            target_company_name, company_files[target_company_name],
            competitor, company_files[competitor],
            llm_config
        )
        if report:
            comparison_reports[comparison_key] = {
                'company1': target_company_name,
                'company2': competitor,
                'report': report
            }
    return comparison_reports

def merge_reports(individual_reports, comparison_reports):
    merged = {}
    for company, report in individual_reports.items():
        merged[company] = report
    for comp_key, comp_data in comparison_reports.items():
        merged[comp_key] = comp_data['report']
    return merged

# ========== 5.1 å•†æ±¤ç§‘æŠ€ä¼°å€¼ä¸é¢„æµ‹åˆ†æ ==========
def get_sensetime_files(data_dir):
    """è·å–å•†æ±¤ç§‘æŠ€çš„è´¢åŠ¡æ•°æ®æ–‡ä»¶"""
    all_files = glob.glob(f"{data_dir}/*.csv")
    sensetime_files = []
    for file in all_files:
        filename = os.path.basename(file)
        company_name = filename.split("_")[0]
        if "å•†æ±¤" in company_name or "SenseTime" in company_name:
            sensetime_files.append(file)
    return sensetime_files

def analyze_sensetime_valuation(files, llm_config):
    """åˆ†æå•†æ±¤ç§‘æŠ€çš„ä¼°å€¼ä¸é¢„æµ‹"""
    query = "åŸºäºä¸‰å¤§è¡¨çš„æ•°æ®ï¼Œæ„å»ºä¼°å€¼ä¸é¢„æµ‹æ¨¡å‹ï¼Œæ¨¡æ‹Ÿå…³é”®å˜é‡å˜åŒ–å¯¹è´¢åŠ¡ç»“æœçš„å½±å“,å¹¶ç»˜åˆ¶ç›¸å…³å›¾è¡¨ã€‚æœ€åç”Ÿæˆæ±‡æŠ¥ç»™æˆ‘ã€‚"
    report = quick_analysis(
        query=query, files=files, llm_config=llm_config, absolute_path=True, max_rounds=20
    )
    return report

# ========== 6. ä¸»ç¨‹åºå…¥å£ ==========
if __name__ == "__main__":
    # å½“å‰å¯ç”¨çš„ä¸»è¦æ•°æ®è¯´æ˜ï¼š
    print("\n========== æ•°æ®è¯´æ˜ ==========")
    print("1. å…¬å¸åŸºç¡€ä¿¡æ¯ï¼ˆæ•´ç†åï¼‰ï¼šcompany_infos\n   ç”¨æ³•ç¤ºä¾‹ï¼šprint(company_infos[:500])  # æ‰“å°å‰500å­—\n")
    print("2. è‚¡æƒä¿¡æ¯åˆ†æï¼ˆæ•´ç†åï¼‰ï¼šshareholder_analysis\n   ç”¨æ³•ç¤ºä¾‹ï¼šprint(shareholder_analysis[:500])\n")
    print("3. è¡Œä¸šä¿¡æ¯æœç´¢ç»“æœï¼ˆæ•´ç†åï¼‰ï¼šsearch_res\n   ç”¨æ³•ç¤ºä¾‹ï¼šprint(search_res[:500])\n")
    print("4. å•å…¬å¸è´¢åŠ¡åˆ†æä¸ä¸¤ä¸¤å¯¹æ¯”åˆ†æç»“æœï¼šmerged_results\n   ç”¨æ³•ç¤ºä¾‹ï¼šprint(format_final_reports(merged_results)[:500])\n")
    print("5. å•†æ±¤ç§‘æŠ€ä¼°å€¼ä¸é¢„æµ‹åˆ†æï¼šsensetime_valuation_report\n   ç”¨æ³•ç¤ºä¾‹ï¼šprint(sensetime_valuation_report['final_report'][:500])\n")
    print("============================\n")

    # è¿è¡Œå…¬å¸åˆ†æ
    results = analyze_companies_in_directory(
        data_directory=data_dir, 
        llm_config=llm_config
    )
    # è¿è¡Œä¸¤ä¸¤å¯¹æ¯”åˆ†æï¼ˆä»¥å•†æ±¤ç§‘æŠ€ä¸ºç›®æ ‡å…¬å¸ï¼‰
    comparison_results = run_comparison_analysis(
        data_directory=data_dir,
        target_company_name=target_company,
        llm_config=llm_config
    )
    # åˆå¹¶æ‰€æœ‰æŠ¥å‘Š
    merged_results = merge_reports(results, comparison_results)

    # å•†æ±¤ç§‘æŠ€ä¼°å€¼ä¸é¢„æµ‹åˆ†æ
    sensetime_files = get_sensetime_files(data_dir)
    sensetime_valuation_report = None
    if sensetime_files:
        sensetime_valuation_report = analyze_sensetime_valuation(sensetime_files, llm_config)

    # æ ¼å¼åŒ–å¹¶è¾“å‡ºæœ€ç»ˆæŠ¥å‘Š
    if merged_results:
        print("\n" + "="*80)
        print("ğŸ“‹ æ ¼å¼åŒ–è´¢åŠ¡æ•°æ®åˆ†ææŠ¥å‘Šï¼ˆå«ä¸¤ä¸¤å¯¹æ¯”ï¼‰")
        print("="*80)
        formatted_report = format_final_reports(merged_results)
        print(formatted_report)
        output_file = f"è´¢åŠ¡åˆ†ææ±‡æ€»æŠ¥å‘Š_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(formatted_report)
        print(f"\nğŸ“ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        # è¾“å‡ºä¼°å€¼åˆ†ææŠ¥å‘Šä¸»è¦å†…å®¹
        if sensetime_valuation_report and isinstance(sensetime_valuation_report, dict):
            print("\n" + "="*80)
            print("ğŸ“Š å•†æ±¤ç§‘æŠ€ä¼°å€¼ä¸é¢„æµ‹åˆ†ææŠ¥å‘Šä¸»è¦å†…å®¹ï¼š")
            print("="*80)
            print(sensetime_valuation_report.get('final_report', 'æœªç”ŸæˆæŠ¥å‘Š'))
        # ç»Ÿä¸€ä¿å­˜ä¸ºmarkdown
        md_output_file = f"è´¢åŠ¡ç ”æŠ¥æ±‡æ€»_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        with open(md_output_file, 'w', encoding='utf-8') as f:
            f.write(f"# å…¬å¸åŸºç¡€ä¿¡æ¯\n\n## æ•´ç†åå…¬å¸ä¿¡æ¯\n\n{company_infos}\n\n")
            f.write(f"# è‚¡æƒä¿¡æ¯åˆ†æ\n\n{shareholder_analysis}\n\n")
            f.write(f"# è¡Œä¸šä¿¡æ¯æœç´¢ç»“æœ\n\n{search_res}\n\n")
            f.write(f"# è´¢åŠ¡æ•°æ®åˆ†æä¸ä¸¤ä¸¤å¯¹æ¯”\n\n{formatted_report}\n\n")
            if sensetime_valuation_report and isinstance(sensetime_valuation_report, dict):
                f.write(f"# å•†æ±¤ç§‘æŠ€ä¼°å€¼ä¸é¢„æµ‹åˆ†æ\n\n{sensetime_valuation_report.get('final_report', 'æœªç”ŸæˆæŠ¥å‘Š')}\n\n")
        print(f"\nğŸ“ Markdownç‰ˆæŠ¥å‘Šå·²ä¿å­˜åˆ°: {md_output_file}")
    else:
        print("\nâŒ æ²¡æœ‰æˆåŠŸåˆ†æçš„å…¬å¸æ•°æ®")

